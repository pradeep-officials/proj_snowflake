-----L-1***************
alter user PRADEEP set default_role = 'SYSADMIN';
alter user PRADEEP set default_warehouse = 'COMPUTE_WH';
alter user PRADEEP set default_namespace = 'UTIL_DB.PUBLIC';

---------------
use role accountadmin;
create or replace api integration dora_api_integration api_provider = aws_api_gateway api_aws_role_arn = 'arn:aws:iam::321463406630:role/snowflakeLearnerAssumedRole' enabled = true api_allowed_prefixes = ('https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora')

show integrations;
-------------------


--CREATE GRADER FUNCTION

use role accountadmin;

create or replace external function util_db.public.grader(        
 step varchar     
 , passed boolean     
 , actual integer     
 , expected integer    
 , description varchar) 
 returns variant 
 api_integration = dora_api_integration 
 context_headers = (current_timestamp, current_account, current_statement, current_account_name) 
 as 'https://awy6hshxy4.execute-api.us-west-2.amazonaws.com/dev/edu_dora/grader'  
;  
        
------------
--CALL FUNCTION
use role accountadmin;

select util_db.public.grader(step, (actual = expected), actual, expected, description) as graded_results from
(SELECT 
 'DORA_IS_WORKING' as step
 ,(select 123 ) as actual
 ,123 as expected
 ,'Dora is working!' as description); 

----------L-2****************
ALTER USER pradeep SET DEFAULT_ROLE = 'SYSADMIN';
create database AGS_GAME_AUDIENCE;
use database AGS_GAME_AUDIENCE;
drop schema AGS_GAME_AUDIENCE.public;
drop schema AGS_GAME_AUDIENCE.AGS_GAME_AUDIENCERAW;
create schema AGS_GAME_AUDIENCE.RAW;

drop table AGS_GAME_AUDIENCE.RAW.GAME_LOGS;

show grants to role ACCOUNTADMIN ;
create storage integration st_int_s3
    type = external_stage
    storage_provider = s3
    storage_aws_role_arn = 'arn:aws:iam::905418410478:role/S3AdminRole'
    enabled = true
    storage_allowed_locations = ( 's3://stg-pradeep' )
    -- storage_blocked_locations = ( 's3://<location1>', 's3://<location2>' )
    -- comment = '<comment>';
SHow INTEGRATIONS;
DESC INTEGRATION st_int_s3;

desc stage stg_pradeep;
desc stage stg_pradeep2;---s3://uni-kishore
list @stg_pradeep2/kickoff;--List the file in stage
list @stg_pradeep2/updated_feed;
select $1 from
@stg_pradeep2
(file_format=>AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS)
--viewing the data in stage
select $1 from
@stg_pradeep
(file_format=>AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS)

--load into table
copy into AGS_GAME_AUDIENCE.RAW.GAME_LOGS
from @stg_pradeep--s3://stg-pradeep
file_format=(format_name=AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS)

---------Copy from s3://uni-kishore for DORA TEST RESTEST for getting BADGE5
create or replace table GAME_LOGS_BKP
clone GAME_LOGS;
---
truncate table GAME_LOGS;

select $1 from
@stg_pradeep2/kickoff
(file_format=>AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS)


copy into AGS_GAME_AUDIENCE.RAW.GAME_LOGS
from @stg_pradeep2/kickoff
file_format=(format_name=AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS);

select count(*) from AGS_GAME_AUDIENCE.RAW.GAME_LOGS

---selecting records after parsing JSON

select RAW_LOG:agent::text as AGENT,
        RAW_LOG:user_event::text as USER_EVENT,
        *
        from GAME_LOGS;

---Creating View
drop view LOGS;
create view LOGS as 
select RAW_LOG:agent::text as AGENT,
        RAW_LOG:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
        RAW_LOG:game_subscription_type::text as game_subscription_type,
        RAW_LOG:user_event::text as USER_EVENT,
        RAW_LOG:user_login::text as user_login,
        *
        from GAME_LOGS;

--selecting from View

select * from LOGS
--------------------------

select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
 SELECT
 'DNGW01' as step
  ,(
      select count(*)  
      from ags_game_audience.raw.logs
      where is_timestamp_ntz(to_variant(datetime_iso8601))= TRUE 
   ) as actual
, 10000 as expected
, 'Project DB and Log File Set Up Correctly' as description
); 


------RETEST*******

-- DO NOT EDIT THIS CODE
select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
 SELECT
 'DNGW01' as step
  ,(
      select count(*)  
      from ags_game_audience.raw.logs
      where is_timestamp_ntz(to_variant(datetime_iso8601))= TRUE 
   ) as actual
, 250 as expected
, 'Project DB and Log File Set Up Correctly' as description
); 
-----------L-3**************
select current_timestamp();
--Change the Time Zone for Your Current Worksheet

--what time zone is your account(and/or session) currently set to? Is it -0700?
select current_timestamp();
alter session set timezone = 'Africa/Nairobi';
select current_timestamp();
alter session set timezone = 'Pacific/Funafuti';
select current_timestamp();

alter session set timezone = 'Asia/Shanghai';
select current_timestamp();
alter session set timezone = 'Asia/Kolkata';
select current_timestamp();
--show the account parameter called timezone
show parameters like 'timezone';

----------Laoding the updated_feed into GAME_LOGS

desc stage stg_pradeep2;---s3://uni-kishore
list @stg_pradeep2/kickoff;--List the file in stage
list @stg_pradeep2/updated_feed;
desc stage stg_pradeep2
--load into table
truncate table AGS_GAME_AUDIENCE.RAW.GAME_LOGS;
copy into AGS_GAME_AUDIENCE.RAW.GAME_LOGS
from @stg_pradeep2
file_format=(format_name=AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS)

select * from GAME_LOGS;
select * from LOGS;


---Creating View
--drop view LOGS;
create or replace view LOGS as 
select --RAW_LOG:agent::text as AGENT,
        RAW_LOG:ip_address::text as ip_address,
        RAW_LOG:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
        RAW_LOG:game_subscription_type::text as game_subscription_type,
        RAW_LOG:user_event::text as USER_EVENT,
        RAW_LOG:user_login::text as user_login,
        *
        from GAME_LOGS where RAW_LOG:ip_address::text is not null;
 --select from view       
select * from LOGS;

--looking for empty AGENT column
select * 
from ags_game_audience.raw.LOGS
where agent is null;

--looking for non-empty IP_ADDRESS column
select 
RAW_LOG:ip_address::text as IP_ADDRESS
,*
from ags_game_audience.raw.LOGS
where RAW_LOG:ip_address::text is not null;

---
select * from LOGS
where user_login ilike '%Prajina%';

select * from LOGS
where user_login like '%prajina%';

---TEST
select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
SELECT
   'DNGW02' as step
   ,( select sum(tally) from(
        select (count(*) * -1) as tally
        from ags_game_audience.raw.logs 
        union all
        select count(*) as tally
        from ags_game_audience.raw.game_logs)     
     ) as actual
   ,250 as expected
   ,'View is filtered' as description
); 
----L-4***********
---finding location from IP

select parse_ip('100.41.16.160','inet');
select parse_ip('100.41.16.160','inet'):host;
select parse_ip('100.41.16.160','inet'):family;
select parse_ip('100.41.16.160','inet'):ipv4;

-----
--Look up Kishore and Prajina's Time Zone in the IPInfo share using his headset's IP Address with the PARSE_IP function.
select start_ip, end_ip, start_ip_int, end_ip_int, city, region, country, timezone
from IPINFO_GEOLOC.demo.location
where parse_ip('100.41.16.160', 'inet'):ipv4 --Kishore's Headset's IP Address
BETWEEN start_ip_int AND end_ip_int;

--Join the log and location tables to add time zone to each row using the PARSE_IP function.
select logs.*
       , loc.city
       , loc.region
       , loc.country
       , loc.timezone
from AGS_GAME_AUDIENCE.RAW.LOGS logs
join IPINFO_GEOLOC.demo.location loc
where parse_ip(logs.ip_address, 'inet'):ipv4 
BETWEEN start_ip_int AND end_ip_int;


--Use two functions supplied by IPShare to help with an efficient IP Lookup Process!
SELECT logs.ip_address
, logs.user_login
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone 
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;


-----convert time zone

--Use two functions supplied by IPShare to help with an efficient IP Lookup Process!
SELECT logs.ip_address
, logs.user_login
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;


---------------
-- Your role should be SYSADMIN
-- Your database menu should be set to AGS_GAME_AUDIENCE
-- The schema should be set to RAW


--a Look Up table to convert from hour number to "time of day name"

drop table if exists ags_game_audience.raw.time_of_day_lu;
create table ags_game_audience.raw.time_of_day_lu
(  hour number
   ,tod_name varchar(25)
);

--insert statement to add all 24 rows to the table
insert into time_of_day_lu
values
(6,'Early morning'),
(7,'Early morning'),
(8,'Early morning'),
(9,'Mid-morning'),
(10,'Mid-morning'),
(11,'Late morning'),
(12,'Late morning'),
(13,'Early afternoon'),
(14,'Early afternoon'),
(15,'Mid-afternoon'),
(16,'Mid-afternoon'),
(17,'Late afternoon'),
(18,'Late afternoon'),
(19,'Early evening'),
(20,'Early evening'),
(21,'Late evening'),
(22,'Late evening'),
(23,'Late evening'),
(0,'Late at night'),
(1,'Late at night'),
(2,'Late at night'),
(3,'Toward morning'),
(4,'Toward morning'),
(5,'Toward morning');


--Check your table to see if you loaded it properly
select tod_name, listagg(hour,',') 
from time_of_day_lu
group by tod_name;

---------Adding Time of day column and calculation logic for it
--Use two functions supplied by IPShare to help with an efficient IP Lookup Process!
SELECT logs.ip_address
, logs.user_login 
, logs.user_event
, logs.datetime_iso8601
, city
, region
, country
, timezone
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
,HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) ltz_hr
,dlu.tod_name
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
join time_of_day_lu dlu on HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601))=dlu.hour
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;

---------RENAMING COLUMNS****
--Use two functions supplied by IPShare to help with an efficient IP Lookup Process!
SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
,HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) ltz_hr
,dlu.tod_name
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
join time_of_day_lu dlu on HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601))=dlu.hour
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;

--CTAS to as ENHANCED table

create table ags_game_audience.enhanced.logs_enhanced as(
 SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
,HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) ltz_hr
,dlu.tod_name
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
join time_of_day_lu dlu on HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601))=dlu.hour
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int
);

---Viewing data from Enhanced table

select * from ags_game_audience.enhanced.logs_enhanced;

---DORA TEST L-4

select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
  SELECT
   'DNGW03' as step
   ,( select count(*) 
      from ags_game_audience.enhanced.logs_enhanced
      where dow_name = 'Sat'
      and tod_name = 'Early evening'   
      and gamer_name like '%prajina'
     ) as actual
   ,2 as expected
   ,'Playing the game on a Saturday evening' as description
); 

----L-5**********************
create or replace task AGS_GAME_AUDIENCE.RAW.LOAD_LOG_ENHANCED
	warehouse=PRADEEP_WH
	schedule='5 minutes'
	as select 'hello';
 --------------------------   
use role accountadmin;
--You have to run this grant or you won't be able to test your tasks while in SYSADMIN role
--this is true even if SYSADMIN owns the task!!
grant execute task on account to role SYSADMIN;

use role sysadmin; 

--Now you should be able to run the task, even if your role is set to SYSADMIN
execute task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

--the SHOW command might come in handy to look at the task 
show tasks in account;

--you can also look at any task more in depth using DESCRIBE
describe task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;


--clone the table to save this version as a backup
--since it holds the records from the UPDATED FEED file, we'll name it _UF
create or replace table ags_game_audience.enhanced.LOGS_ENHANCED_UF 
clone ags_game_audience.enhanced.LOGS_ENHANCED;
------------

truncate  table AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED ;
create or replace task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED
	warehouse=PRADEEP_WH
	schedule='5 minutes'
	as INSERT INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED 
     SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
,HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) ltz_hr
,dlu.tod_name
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
join time_of_day_lu dlu on HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601))=dlu.hour
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int;
----------------
select * from ags_game_audience.enhanced.LOGS_ENHANCED ;
select * from ags_game_audience.enhanced.LOGS_ENHANCED_UF ;


--------------------Insert MERGE
---truncate  table AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED ;
create or replace task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED
	warehouse=PRADEEP_WH
	schedule='5 minutes'
	as 
    MERGE INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED e
    using
    (
     SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
,HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) ltz_hr
,dlu.tod_name
from AGS_GAME_AUDIENCE.RAW.LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
join time_of_day_lu dlu on HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601))=dlu.hour
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int)r
ON  e.GAMER_NAME =r.GAMER_NAME
and e.GAME_EVENT_NAME= r.GAME_EVENT_NAME
and e.GAME_EVENT_UTC = r.GAME_EVENT_UTC
WHEN NOT MATCHED THEN
INSERT(IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, LTZ_HR, TOD_NAME) 
VALUES (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, LTZ_HR, TOD_NAME) ;

-------Verifying Merge Insert
select * from ags_game_audience.enhanced.LOGS_ENHANCED ;
select * from ags_game_audience.enhanced.LOGS_ENHANCED_UF 

---------TEsting the MERGE with some dummy records:

--Insert a test record into your Raw Table 
--You can change the user_event field each time to create "new" records 
--editing the ip_address or datetime_iso8601 can complicate things more than they need to 
--editing the user_login will make it harder to remove the fake records after you finish testing 
INSERT INTO ags_game_audience.raw.game_logs 
select PARSE_JSON('{"datetime_iso8601":"2025-01-01 00:00:00.000", "ip_address":"196.197.196.255", "user_event":"fake event", "user_login":"fake user"}');

--After inserting a new row, run the Merge again 
EXECUTE TASK AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED;

--Check to see if any rows were added 
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;


delete from ags_game_audience.raw.game_logs where raw_log like '%fake user%';

--delete the fake rows from the enhanced table
delete from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED
where gamer_name = 'fake user';

--Row count should be back to what it was in the beginning
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED; 

-------DORA TEST L-5 *********
select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
SELECT
'DNGW04' as step
 ,( select count(*)/iff (count(*) = 0, 1, count(*))
  from table(ags_game_audience.information_schema.task_history
              (task_name=>'LOAD_LOGS_ENHANCED'))) as actual
 ,1 as expected
 ,'Task exists and has been run at least once' as description 
 ); 

------L-6*******************

--- Create A New Stage using wizzard
list@UNI_KISHORE_PIPELINE
--create new table 
create or replace TABLE AGS_GAME_AUDIENCE.RAW.PL_GAME_LOGS (
	RAW_LOG VARIANT
);

 ---copy data into PL_GAME_LOGS Tabls using stage 

 COPY INTO AGS_GAME_AUDIENCE.RAW.PL_GAME_LOGS
 FROM  @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
file_format=(format_name=AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS);

----
select * from PL_GAME_LOGS;

---creating a tsk to load file from UNI_KISHORE_PIPELINE bucket every 10 min

create or replace task GET_NEW_FILES 
warehouse=PRADEEP_WH
	schedule='10 minutes'
	as COPY INTO AGS_GAME_AUDIENCE.RAW.PL_GAME_LOGS
 FROM  @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
file_format=(format_name=AGS_GAME_AUDIENCE.RAW.FF_JSON_LOGS);
---------------
select * from PL_GAME_LOGS;
---Change the status of the task 
ALTER TASK IF EXISTS  GET_NEW_FILES SUSPEND-- RESUME
execute task GET_NEW_FILES ;
describe task GET_NEW_FILES;


----------Creating View from RAW table and creating task to automatically load data into the final enhanced table.

--Craeting VIEW:----------------------
 Create or replace view PL_LOGS as
select --RAW_LOG:agent::text as AGENT,
        RAW_LOG:ip_address::text as ip_address,
        RAW_LOG:datetime_iso8601::TIMESTAMP_NTZ as datetime_iso8601,
        RAW_LOG:game_subscription_type::text as game_subscription_type,
        RAW_LOG:user_event::text as USER_EVENT,
        RAW_LOG:user_login::text as user_login,
        *
        from PL_GAME_LOGS where RAW_LOG:ip_address::text is not null;
-------- -----------------------
select * from PL_LOGS;
-------------        
--drop view PL_LOGS;

select * from PL_LOGS;
--------------------Insert MERGE--------------

---truncate  table AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED ;
create or replace task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED 
	warehouse=PRADEEP_WH
	schedule='10 minutes'
	as 
    MERGE INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED e
    using
    (
      SELECT logs.ip_address
, logs.user_login as GAMER_NAME
, logs.user_event as GAME_EVENT_NAME
, logs.datetime_iso8601 as GAME_EVENT_UTC
, city
, region
, country
, timezone as GAMER_LTZ_NAME
,CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601) as game_event_ltz
,DAYNAME(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) as DOW_NAME
,HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601)) ltz_hr
,dlu.tod_name
from AGS_GAME_AUDIENCE.RAW.PL_LOGS logs
JOIN IPINFO_GEOLOC.demo.location loc 
ON IPINFO_GEOLOC.public.TO_JOIN_KEY(logs.ip_address) = loc.join_key
join time_of_day_lu dlu on HOUR(CONVERT_TIMEZONE('UTC',timezone,logs.datetime_iso8601))=dlu.hour
AND IPINFO_GEOLOC.public.TO_INT(logs.ip_address) 
BETWEEN start_ip_int AND end_ip_int)r
ON  e.GAMER_NAME =r.GAMER_NAME
and e.GAME_EVENT_NAME= r.GAME_EVENT_NAME
and e.GAME_EVENT_UTC = r.GAME_EVENT_UTC
WHEN NOT MATCHED THEN
INSERT(IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, LTZ_HR, TOD_NAME) 
VALUES (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME, GAME_EVENT_UTC, CITY, REGION, COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ, DOW_NAME, LTZ_HR, TOD_NAME) ;


-------------verifying the data


---Change the status of the task 
ALTER TASK IF EXISTS  GET_NEW_FILES SUSPEND-- RESUME
describe task GET_NEW_FILES;
describe task LOAD_LOGS_ENHANCED;

execute task GET_NEW_FILES ;
execute task LOAD_LOGS_ENHANCED ;
select * from PL_GAME_LOGS;
select * from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;
-------------------------------------------

--SHOW TASKS 

show tasks in account;

-------------------
--Turning on a task is done with a RESUME command
alter task AGS_GAME_AUDIENCE.RAW.GET_NEW_FILES resume;
alter task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED resume;

--Turning OFF a task is done with a SUSPEND command
alter task AGS_GAME_AUDIENCE.RAW.GET_NEW_FILES suspend;
alter task AGS_GAME_AUDIENCE.RAW.LOAD_LOGS_ENHANCED suspend;


-----------===========================TESTING

--Step 1 - how many files in the bucket?
list @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE;

--Step 2 - number of rows in raw table (should be file count x 10)
select count(*) from AGS_GAME_AUDIENCE.RAW.PL_GAME_LOGS;

--Step 3 - number of rows in raw table (should be file count x 10)
select count(*) from AGS_GAME_AUDIENCE.RAW.PL_LOGS;

--Step 4 - number of rows in enhanced table (should be file count x 10 but fewer rows is okay because not all IP addresses are available from the IPInfo share)
select count(*) from AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED;

---***********To use the SERVERLESS task mode, we'll need to grant that privilege to SYSADMIN. 
-----Grant Serverless Task Management to SYSADMIN

use role accountadmin;
grant EXECUTE MANAGED TASK on account to SYSADMIN;

--switch back to sysadmin
use role sysadmin;

-------------

----DORA TEST

select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
SELECT
'DNGW05' as step
 ,(
   select max(tally) from (
       select CASE WHEN SCHEDULED_FROM = 'SCHEDULE' 
                         and STATE= 'SUCCEEDED' 
              THEN 1 ELSE 0 END as tally 
   from table(ags_game_audience.information_schema.task_history (task_name=>'GET_NEW_FILES')))
  ) as actual
 ,1 as expected
 ,'Task succeeds from schedule' as description
 ); 

 ------------L-7******************************
  SELECT 
    METADATA$FILENAME as log_file_name --new metadata column
  , METADATA$FILE_ROW_NUMBER as log_file_row_id --new metadata column
  , current_timestamp(0) as load_ltz --new local time of load
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
  (file_format => 'ff_json_logs');

  --------------------------
  Create table ED_PIPELINE_LOGS AS
   SELECT 
    METADATA$FILENAME as log_file_name --new metadata column
  , METADATA$FILE_ROW_NUMBER as log_file_row_id --new metadata column
  , current_timestamp(0) as load_ltz --new local time of load
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
  (file_format => 'ff_json_logs');

 --- drop table ED_PIPELINE_LOGS

 --reload the table using your COPY INTO
COPY INTO ED_PIPELINE_LOGS
FROM (
    SELECT 
    METADATA$FILENAME as log_file_name 
  , METADATA$FILE_ROW_NUMBER as log_file_row_id 
  , current_timestamp(0) as load_ltz 
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
)
file_format = (format_name = ff_json_logs);

--select * from ED_PIPELINE_LOGS;

----Create  Snowpipe!

CREATE OR REPLACE PIPE PIPE_GET_NEW_FILES
auto_ingest=true
aws_sns_topic='arn:aws:sns:us-west-2:321463406630:dngw_topic'
AS 
COPY INTO ED_PIPELINE_LOGS
FROM (
    SELECT 
    METADATA$FILENAME as log_file_name 
  , METADATA$FILE_ROW_NUMBER as log_file_row_id 
  , current_timestamp(0) as load_ltz 
  , get($1,'datetime_iso8601')::timestamp_ntz as DATETIME_ISO8601
  , get($1,'user_event')::text as USER_EVENT
  , get($1,'user_login')::text as USER_LOGIN
  , get($1,'ip_address')::text as IP_ADDRESS    
  FROM @AGS_GAME_AUDIENCE.RAW.UNI_KISHORE_PIPELINE
)
file_format = (format_name = ff_json_logs);


---------------------Update the LOAD_LOGS_ENHANCED Task************
---Create backup of LOGS_ENHANCED Tabls
Create or replace table LOGS_ENHANCED_BACKUP
clone LOGS_ENHANCED;

--truncate table LOGS_ENHANCED;
--truncate table ED_PIPELINE_LOGS;
select * from LOGS_ENHANCED;
select * from ED_PIPELINE_LOGS;

----------------If snow pipe is not pulling data and looks like it is stalled out 
--use below commnd:

ALTER PIPE ags_game_audience.raw.PIPE_GET_NEW_FILES REFRESH;

---command to check if pipe is running:

select parse_json(SYSTEM$PIPE_STATUS( 'ags_game_audience.raw.PIPE_GET_NEW_FILES' ));


--------------------------------
--Create STREAM


--create a stream that will keep track of changes to the table
create or replace stream ags_game_audience.raw.ed_cdc_stream 
on table AGS_GAME_AUDIENCE.RAW.ED_PIPELINE_LOGS;

--look at the stream you created
show streams;

--check to see if any changes are pending (expect FALSE the first time you run it)
--after the Snowpipe loads a new file, expect to see TRUE
select system$stream_has_data('ed_cdc_stream');


--------View Our Stream Data

--query the stream
select * 
from ags_game_audience.raw.ed_cdc_stream; 

--check to see if any changes are pending
select system$stream_has_data('ed_cdc_stream');

--if your stream remains empty for more than 10 minutes, make sure your PIPE is running
select SYSTEM$PIPE_STATUS('PIPE_GET_NEW_FILES');

--if you need to pause or unpause your pipe
--alter pipe PIPE_GET_NEW_FILES set pipe_execution_paused = true;
--alter pipe PIPE_GET_NEW_FILES set pipe_execution_paused = false;


----------------Process the Rows from the Stream

--make a note of how many rows are in the stream
select * 
from ags_game_audience.raw.ed_cdc_stream; 

 
--process the stream by using the rows in a merge 
MERGE INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED e
USING (
        SELECT cdc.ip_address 
        , cdc.user_login as GAMER_NAME
        , cdc.user_event as GAME_EVENT_NAME
        , cdc.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,cdc.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_game_audience.raw.ed_cdc_stream cdc
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(cdc.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(cdc.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN AGS_GAME_AUDIENCE.RAW.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
      ) r
ON r.GAMER_NAME = e.GAMER_NAME
AND r.GAME_EVENT_UTC = e.GAME_EVENT_UTC
AND r.GAME_EVENT_NAME = e.GAME_EVENT_NAME 
WHEN NOT MATCHED THEN 
INSERT (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME)
        VALUES
        (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME);

 

--Did all the rows from the stream disappear? 
select * 
from ags_game_audience.raw.ed_cdc_stream; 


--------------Create a CDC-Fueled, Time-Driven Task


--Create a new task that uses the MERGE you just tested
create or replace task AGS_GAME_AUDIENCE.RAW.CDC_LOAD_LOGS_ENHANCED
	USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE='XSMALL'
	SCHEDULE = '5 minutes'
	as 
MERGE INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED e
USING (
        SELECT cdc.ip_address 
        , cdc.user_login as GAMER_NAME
        , cdc.user_event as GAME_EVENT_NAME
        , cdc.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,cdc.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_game_audience.raw.ed_cdc_stream cdc
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(cdc.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(cdc.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN AGS_GAME_AUDIENCE.RAW.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
      ) r
ON r.GAMER_NAME = e.GAMER_NAME
AND r.GAME_EVENT_UTC = e.GAME_EVENT_UTC
AND r.GAME_EVENT_NAME = e.GAME_EVENT_NAME 
WHEN NOT MATCHED THEN 
INSERT (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME)
        VALUES
        (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME);
        
--Resume the task so it is running
alter task AGS_GAME_AUDIENCE.RAW.CDC_LOAD_LOGS_ENHANCED resume;

------------Add A Stream Dependency to the Task Schedule
--Add STREAM dependency logic to the TASK header and replace the task. 

create or replace task AGS_GAME_AUDIENCE.RAW.CDC_LOAD_LOGS_ENHANCED
	USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE='XSMALL'
	SCHEDULE = '5 minutes'
    WHEN  system$stream_has_data('ed_cdc_stream')
	as 
MERGE INTO AGS_GAME_AUDIENCE.ENHANCED.LOGS_ENHANCED e
USING (
        SELECT cdc.ip_address 
        , cdc.user_login as GAMER_NAME
        , cdc.user_event as GAME_EVENT_NAME
        , cdc.datetime_iso8601 as GAME_EVENT_UTC
        , city
        , region
        , country
        , timezone as GAMER_LTZ_NAME
        , CONVERT_TIMEZONE( 'UTC',timezone,cdc.datetime_iso8601) as game_event_ltz
        , DAYNAME(game_event_ltz) as DOW_NAME
        , TOD_NAME
        from ags_game_audience.raw.ed_cdc_stream cdc
        JOIN ipinfo_geoloc.demo.location loc 
        ON ipinfo_geoloc.public.TO_JOIN_KEY(cdc.ip_address) = loc.join_key
        AND ipinfo_geoloc.public.TO_INT(cdc.ip_address) 
        BETWEEN start_ip_int AND end_ip_int
        JOIN AGS_GAME_AUDIENCE.RAW.TIME_OF_DAY_LU tod
        ON HOUR(game_event_ltz) = tod.hour
      ) r
ON r.GAMER_NAME = e.GAMER_NAME
AND r.GAME_EVENT_UTC = e.GAME_EVENT_UTC
AND r.GAME_EVENT_NAME = e.GAME_EVENT_NAME 
WHEN NOT MATCHED THEN 
INSERT (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME)
        VALUES
        (IP_ADDRESS, GAMER_NAME, GAME_EVENT_NAME
        , GAME_EVENT_UTC, CITY, REGION
        , COUNTRY, GAMER_LTZ_NAME, GAME_EVENT_LTZ
        , DOW_NAME, TOD_NAME);

--Resume the task so it is running
alter task AGS_GAME_AUDIENCE.RAW.CDC_LOAD_LOGS_ENHANCED resume;

---------DORA TEST for L-8

select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
SELECT
'DNGW06' as step
 ,(
   select CASE WHEN pipe_status:executionState::text = 'RUNNING' THEN 1 ELSE 0 END 
   from(
   select parse_json(SYSTEM$PIPE_STATUS( 'ags_game_audience.raw.PIPE_GET_NEW_FILES' )) as pipe_status)
  ) as actual
 ,1 as expected
 ,'Pipe exists and is RUNNING' as description
 ); 


 ---------PAUSE PIPE
 alter pipe PIPE_GET_NEW_FILES set pipe_execution_paused = true;
 alter task AGS_GAME_AUDIENCE.RAW.CDC_LOAD_LOGS_ENHANCED suspend;

 ------------DORA TEST L-9

 select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
SELECT
'DNGW07' as step
 ,(
   select 1 
  ) as actual
 ,1 as expected
 ,'We hope you learned about dashboards' as description
 ); 


 -----------L-9***********
 --Create DATABASE CURATED;
 --DROP DATABASE CURATED;
  Create SCHEMA CURATED;
  --DROP SCHEMA CURATED;

